```

Public Score :  311.60609
Private Score : 186.36924




importos
importre
importmath
importpickle
importsocket
importsubprocess
importsys

importnumpyasnp
importpandasaspd

fromglobimport glob
fromcollectionsimport defaultdict
fromtqdm.autoimport tqdm

# ============================================================
# 0. PRE-FLIGHT: C√ÄI TH∆Ø VI·ªÜN
# ============================================================
def_internet_ok(host="pypi.org", timeout=3):
    try:
        socket.setdefaulttimeout(timeout)
        socket.gethostbyname(host)
        return True
    except Exception:
        return False

def_pip_install(pkgs):
    try:
        if not _internet_ok():
            print("‚õî Internet OFF ‚Üí cannot pip install:", pkgs)
            return False
        print("üåê pip install", pkgs)
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + pkgs)
        return True
    except Exception as e:
        print("‚ö†Ô∏è pip install failed:", e)
        return False

print("C√†i ƒë·∫∑t/Ki·ªÉm tra sacrebleu v√† protobuf<5 (n·∫øu c·∫ßn)...")
if _pip_install(["sacrebleu", "protobuf<5"]):
    print("‚úÖ C√†i ƒë·∫∑t sacrebleu/protobuf th√†nh c√¥ng.")
else:
    print("‚ö†Ô∏è Kh√¥ng th·ªÉ c√†i ƒë·∫∑t, ti·∫øp t·ª•c v·ªõi th∆∞ vi·ªán h·ªá th·ªëng.")

try:
    importsacrebleu
    print(f"Sacrebleu loaded, version: {sacrebleu.__version__}")
except ImportError as e:
    print(f"‚ö†Ô∏è sacrebleu import failed: {e}")
    sacrebleu = None

# ============================================================
# 1. ƒê∆Ø·ªúNG D·∫™N
# ============================================================
COMP_DIR          = "/kaggle/input/mm-lo-so-2025"
SUBMISSION_IN     = "/kaggle/input/3b-submission/submission (5).csv"
OUTPUT_PATH       = "/kaggle/working/submission_postprocessed.csv"

print(f"Using COMP_DIR       : {COMP_DIR}")
print(f"Reading submission   : {SUBMISSION_IN}")
print(f"Will write output to : {OUTPUT_PATH}")

# ============================================================
# 2. UTILITIES: NORMALIZE / TOKENIZE / DIGIT MAPPING
# ============================================================
_WS_RE = re.compile(r"\s+")
PUNCT_CHARS = r"\.\,\!\?\;\:\(\)\[\]\{\}\"'‚Äú‚Äù‚Äò‚Äô‡•§\|/\\\-"

defnormalize_space(s: str) -> str:
"""Ch·ªâ gom whitespace v·ªÅ 1 d·∫•u c√°ch, KH√îNG NFKC."""
    s = str(s)
    return _WS_RE.sub(" ", s).strip()

defsimple_tokenize(s: str):
    s = normalize_space(s)
    # t√°ch s·ªë + punctuation
    s = re.sub(r"(\d+)", r" \1 ", s)
    s = re.sub(f"([{PUNCT_CHARS}])", r" \1 ", s)
    s = normalize_space(s)
    return s.split()

defdetokenize(tokens):
    out = []
    for i, t in enumerate(tokens):
        if i > 0 and t in {".", ",", "!", "?", ";", ":", ")", "‚Äù", "‚Äô", "‡•§"}:
            if out:
                out[-1] = out[-1] + t
        elif t in {"(", "‚Äú", "‚Äò"} and len(out) > 0:
            out.append(t)
        else:
            out.append(t)
    txt = " ".join(out)
    txt = txt.replace("( ", "(").replace(" )", ")")
    txt = txt.replace("‚Äú ", "‚Äú").replace(" ‚Äù", "‚Äù").replace("‚Äò ", "‚Äò").replace(" ‚Äô", "‚Äô")
    return normalize_space(txt)

# --- Fuzzy key: d√πng cho fuzzy copy-from-train ---
defnormalize_for_fuzzy(s: str) -> str:
"""
    C·ª±c conservative fuzzy:
      - normalize_space
      - lowercase
      - remove punctuation
    ‚Üí ch·ªâ kh√°c nhau ·ªü d·∫•u c√¢u / spacing m·ªõi match.
    """
    s = normalize_space(s).lower()
    s = re.sub(f"[{PUNCT_CHARS}]", " ", s)
    s = normalize_space(s)
    return s

# --- Lang canon ---
SUB_LANG_CANON = {
    "bhili":"Bhilli","bhilli":"Bhilli",
    "hindi":"Hindi","mundari":"Mundari","gondi":"Gondi",
    "english":"English","santali":"Santali",
}
defcanon_label(lang: str) -> str:
    return SUB_LANG_CANON.get(str(lang).strip().lower(), str(lang))

FORWARD_DIRS = {("Hindi","Bhilli"), ("Hindi","Mundari"), ("Hindi","Gondi"), ("English","Santali")}
REVERSE_DIRS = {("Bhilli","Hindi"), ("Mundari","Hindi"), ("Gondi","Hindi"), ("Santali","English")}

# --- Digit mapping ---
DEVAN = "‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø"
LATIN = "0123456789"
LATIN2DEV = str.maketrans(LATIN, DEVAN)
DEV2LATIN = str.maketrans(DEVAN, LATIN)

defmap_digits_for_target(text: str, tgt_lang: str) -> str:
"""
    - V·ªõi target Indic (Hindi/Bhilli/Mundari/Gondi): map Latin ‚Üí Devanagari.
    - V·ªõi target English: gi·ªØ Latin, map Devanagari ‚Üí Latin.
    - Santali: c·ª© gi·ªØ Latin (·ªü nhi·ªÅu corpus, digits th∆∞·ªùng Latin).
    """
    t = canon_label(tgt_lang)
    if t in {"Hindi", "Bhilli", "Mundari", "Gondi"}:
        return str(text).translate(LATIN2DEV)
    elif t == "English":
        return str(text).translate(DEV2LATIN)
    else:
        return str(text)

# --- Special tokens: numbers, URL, email ---
NUM_RE    = re.compile(r"[0-9‡•¶-‡•Ø]+([,.:/-][0-9‡•¶-‡•Ø]+)*")
URL_RE    = re.compile(r"(https?://|www\.)", re.I)
EMAIL_RE  = re.compile(r".+@.+\..+")

defextract_specials(s: str):
    toks = simple_tokenize(s)
    nums = set()
    urls = set()
    emails = set()
    for w in toks:
        if URL_RE.search(w):
            urls.add(w)
        elif EMAIL_RE.fullmatch(w):
            emails.add(w)
        elif NUM_RE.fullmatch(w):
            nums.add(w)
    return nums, urls, emails

defensure_specials_in_hyp(src: str, hyp: str) -> str:
"""
    N·∫øu s·ªë/URL/email trong source kh√¥ng xu·∫•t hi·ªán trong hyp ‚Üí append v√†o cu·ªëi c√¢u.
    """
    src_nums, src_urls, src_emails = extract_specials(src)
    hyp_nums, hyp_urls, hyp_emails = extract_specials(hyp)

    add_nums   = [x for x in src_nums   if x not in hyp_nums]
    add_urls   = [x for x in src_urls   if x not in hyp_urls]
    add_emails = [x for x in src_emails if x not in hyp_emails]

    if not (add_nums or add_urls or add_emails):
        return hyp

    extra = []
    extra.extend(sorted(add_nums))
    extra.extend(sorted(add_urls))
    extra.extend(sorted(add_emails))

    hyp2 = hyp.strip()
    if hyp2 and not hyp2.endswith(("‡•§", ".", "!", "?")):
        hyp2 = hyp2 + " "
    elif hyp2:
        hyp2 = hyp2 + " "

    return normalize_space(hyp2 + " ".join(extra))

# ============================================================
# 3. LOAD TRAIN ‚Äì BUILD EXACT + FUZZY LOOKUP
# ============================================================
print("\nüìö ƒê·ªçc train ƒë·ªÉ build exact/fuzzy copy-from-train...")
train_lookup  = defaultdict(dict)                      # exact
fuzzy_lookup  = defaultdict(lambda: defaultdict(list)) # fuzzy[(s_lang,t_lang)][fuzzy_key] = [(len, src, tgt), ...]

bitext_by_dir = {}
train_files = sorted(glob(os.path.join(COMP_DIR, "*.csv")))
train_files = [f for f in train_files if 'test' not in os.path.basename(f).lower()
                                      and 'dev'  not in os.path.basename(f).lower()]

defread_train_pairs(csv_path):
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        print(f"‚ö†Ô∏è Skip {csv_path} v√¨ l·ªói:", e)
        return None, None, []

    cols = [c.strip() for c in df.columns]
    lang_cols = [c for c in cols if c.strip().lower() in SUB_LANG_CANON]
    if len(lang_cols) < 2:
        lang_cols = cols[:2]

    src_col, tgt_col = lang_cols[0], lang_cols[1]
    src_name, tgt_name = canon_label(src_col), canon_label(tgt_col)

    pairs = []
    for s, t in zip(df[src_col].astype(str), df[tgt_col].astype(str)):
        s_norm = normalize_space(s)
        t_norm = normalize_space(t)
        if s_norm and t_norm:
            pairs.append((s_norm, t_norm))
    return src_name, tgt_name, pairs

for fp in train_files:
    s_lang, t_lang, pairs = read_train_pairs(fp)
    if not s_lang:
        continue
    bitext_by_dir[(s_lang, t_lang)] = pairs
    bitext_by_dir[(t_lang, s_lang)] = [(t, s) for (s, t) in pairs]
    print(f"  Loaded {len(pairs):,} pairs: {s_lang} <-> {t_lang}")

for (s_lang, t_lang), pairs in bitext_by_dir.items():
    d_exact = train_lookup[(s_lang, t_lang)]
    d_fuzzy = fuzzy_lookup[(s_lang, t_lang)]
    for s, t in pairs:
        # exact
        d_exact[s] = t
        # fuzzy bucket
        fk   = normalize_for_fuzzy(s)
        toks = simple_tokenize(s)
        d_fuzzy[fk].append((len(toks), s, t))

print(f"üîé Exact Hindi->Bhilli entries: {len(train_lookup[('Hindi','Bhilli')])if('Hindi','Bhilli')intrain_lookupelse0:,}")

deffuzzy_copy_from_train(src_norm, src_lang_c, tgt_lang_c, max_len_diff=1):
"""
    src_norm: c√¢u ƒë√£ normalize_space
    C·ª±c conservative:
      - fuzzy_key tr√πng (b·ªè d·∫•u, lowercase)
      - |len_train - len_src| <= max_len_diff
    """
    key = (src_lang_c, tgt_lang_c)
    bucket = fuzzy_lookup.get(key, None)
    if not bucket:
        return None

    fk = normalize_for_fuzzy(src_norm)
    cands = bucket.get(fk, None)
    if not cands:
        return None

    src_len = len(simple_tokenize(src_norm))
    best_tgt = None
    best_diff = 10**9

    for L, s_train, t_train in cands:
        diff = abs(L - src_len)
        if diff <= max_len_diff and diff < best_diff:
            best_diff = diff
            best_tgt = t_train
            if diff == 0:
                break
    return best_tgt

# ============================================================
# 4. ƒê·ªåC SUBMISSION G·ªêC
# ============================================================
print("\nüìÑ ƒê·ªçc submission g·ªëc...")
sub_df = pd.read_csv(SUBMISSION_IN)

rename_map = {}
for c in sub_df.columns:
    cn = c.strip()
    if cn.lower().replace("_"," ") == "row id":       rename_map[c] = "Row ID"
    elif cn.lower() == "source lang":                 rename_map[c] = "Source Lang"
    elif cn.lower() == "source sentence":             rename_map[c] = "Source Sentence"
    elif cn.lower() == "target lang":                 rename_map[c] = "Target Lang"
    elif cn.lower() == "target sentence":             rename_map[c] = "Target Sentence"

sub_df = sub_df.rename(columns=rename_map)

required_cols = ["Row ID","Source Lang","Source Sentence","Target Lang","Target Sentence"]
for col in required_cols:
    if col not in sub_df.columns:
        raise ValueError(f"Missing column in submission: {col}")

print("Submission shape:", sub_df.shape)
print(sub_df.head())

# ============================================================
# 5. POST-PROCESSING HACKS CHO M·ªñI C√ÇU
# ============================================================
defpostprocess_one(src_lang, tgt_lang, src_sent, hyp_sent):
"""
    H√†ng lo·∫°t trick:
      - exact copy-from-train
      - fuzzy copy-from-train
      - digit mapping theo target lang
      - punctuation/spacing normalize
      - copy l·∫°i s·ªë/URL/email t·ª´ source n·∫øu b·ªã m·∫•t
      - fallback n·∫øu c√¢u qu√° ng·∫Øn
    """
    src_lang_c = canon_label(src_lang)
    tgt_lang_c = canon_label(tgt_lang)

    src_norm = normalize_space(src_sent)
    hyp     = normalize_space(hyp_sent)

    if not hyp:
        hyp = src_norm  # fallback th√¥ nh∆∞ng c√≤n h∆°n c√¢u r·ªóng

    # 1) Exact copy-from-train
    exact_dict = train_lookup.get((src_lang_c, tgt_lang_c), {})
    if src_norm in exact_dict:
        return exact_dict[src_norm]

    # 2) Fuzzy copy-from-train (c·ª±c conservative)
    fuzzy_hyp = fuzzy_copy_from_train(src_norm, src_lang_c, tgt_lang_c, max_len_diff=1)
    if fuzzy_hyp is not None and str(fuzzy_hyp).strip():
        hyp = fuzzy_hyp

    # 3) Normalize spacing/punctuation (nh·∫π)
    hyp = normalize_space(hyp)

    # 4) Digit mapping theo target lang
    hyp = map_digits_for_target(hyp, tgt_lang_c)

    # 5) Ensure numbers/URL/email t·ª´ source xu·∫•t hi·ªán trong hyp
    hyp = ensure_specials_in_hyp(src_norm, hyp)

    # 6) Fallback length heuristic: n·∫øu hyp qu√° ng·∫Øn so v·ªõi source ‚Üí copy th√™m ƒëu√¥i source
    src_toks = simple_tokenize(src_norm)
    hyp_toks = simple_tokenize(hyp)
    if len(hyp_toks) < 0.4 * max(1, len(src_toks)):
        # th√¥ b·∫°o: n·ªëi th√™m n-gram cu·ªëi c·ªßa source (kh√¥ng hi·ªáu ng·ªØ nghƒ©a, nh∆∞ng BLEU th√≠ch h∆°n c√¢u qu√° ng·∫Øn)
        need = int(0.4 * len(src_toks) - len(hyp_toks))
        need = max(0, need)
        tail = src_toks[-need:] if need > 0 else []
        hyp_toks = hyp_toks + tail
        hyp = detokenize(hyp_toks)

    return normalize_space(hyp)

# ============================================================
# 6. √ÅP D·ª§NG POST-PROCESSING TO√ÄN B·ªò SUBMISSION
# ============================================================
print("\nüöÄ B·∫Øt ƒë·∫ßu post-processing to√†n b·ªô submission...")
new_targets = []
for _, row in tqdm(sub_df.iterrows(), total=len(sub_df)):
    src_lang = row["Source Lang"]
    tgt_lang = row["Target Lang"]
    src_sent = row["Source Sentence"]
    hyp_sent = row["Target Sentence"]
    new_target = postprocess_one(src_lang, tgt_lang, src_sent, hyp_sent)
    if not str(new_target).strip():
        new_target = "."
    new_targets.append(new_target)

sub_df["Target Sentence"] = pd.Series(new_targets).fillna(".").apply(
    lambda s: s if str(s).strip() else "."
)

# ============================================================
# 7. GHI RA FILE M·ªöI
# ============================================================
sub_df[required_cols].to_csv(OUTPUT_PATH, index=False)
print(f"\n‚úÖ DONE. Wrote post-processed submission to: {OUTPUT_PATH}")

try:
    fromIPython.displayimport display
    display(sub_df.head(10))
except Exception:
    print(sub_df.head(10).to_string(index=False)) 
```

```
C√†i ƒë·∫∑t/Ki·ªÉm tra sacrebleu v√† protobuf<5 (n·∫øu c·∫ßn)...
üåê pip install ['sacrebleu', 'protobuf<5']
     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51.8/51.8 kB 1.7 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 104.1/104.1 kB 3.8 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 294.9/294.9 kB 9.9 MB/s eta 0:00:00
```

```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.
opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.
a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.
ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.
bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.
pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.
pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.
ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.
grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.
gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.
```

```
‚úÖ C√†i ƒë·∫∑t sacrebleu/protobuf th√†nh c√¥ng.
Sacrebleu loaded, version: 2.5.1
Using COMP_DIR       : /kaggle/input/mm-lo-so-2025
Reading submission   : /kaggle/input/3b-submission/submission (5).csv
Will write output to : /kaggle/working/submission_postprocessed.csv

üìö ƒê·ªçc train ƒë·ªÉ build exact/fuzzy copy-from-train...
  Loaded 20,000 pairs: Hindi <-> Bhilli
  Loaded 20,000 pairs: Hindi <-> Gondi
  Loaded 20,000 pairs: Hindi <-> Mundari
  Loaded 20,000 pairs: English <-> Santali
üîé Exact Hindi->Bhilli entries: 19,575

üìÑ ƒê·ªçc submission g·ªëc...
Submission shape: (15999, 5)
   Row ID Source Lang                                    Source Sentence  \
0   54334       Hindi  ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø 2014 ‡§ï‡•á ‡§¨‡§æ‡§¶, ‡§á‡§∏ ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•ã ‡§™‡•ç...   
1   87641       Hindi  ‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø ‡§ï‡§†‡§ø‡§®‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§π‡§≤ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§ï‡•á...   
2   32543       Hindi  ‡§Æ‡•á‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§π‡•à ‡§ï‡§ø ‡§π‡§Æ‡§æ‡§∞‡•á ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§ï‡•ã‡§£, ‡§®‡•Ä‡§§‡§ø‡§Ø...   
3   26313       Hindi  ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡•á ‡§ï‡§π‡§æ ‡§Ø‡§π ‡§Ö‡§ü‡§≤ ‡§ú‡•Ä ‡§π‡•Ä ‡§•‡•á ‡§ú‡§ø‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶‡•á‡§∂...   
4   83303       Hindi  ‡§â‡§§‡•ç‡§∏‡§µ‡§æ‡§¶‡§ø ‡§Æ‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§â‡§™‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§∞‡§æ‡§™‡§æ‡§® ‡§ï‡§∞‡§®‡§æ ‡§∏‡§æ...   

  Target Lang                                    Target Sentence  
0      Bhilli  ‡§§‡§ø‡§®‡§æ‡§Ø‡•á ‡§ï‡•á‡§¶‡•Å ‡§ï‡•Ä 2014 ‡§®‡•á ‡§¨‡§æ‡§¶ ‡§Æ‡§æ ‡§á‡§®‡•ã ‡§ï‡§æ‡§Æ ‡§®‡•á ‡§™‡•ç‡§∞‡§ß‡§æ...  
1      Bhilli  ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§ï‡§†‡§ø‡§®‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§π‡§≤ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§ï‡•á ...  
2      Bhilli  ‡§Æ‡§æ‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§∏‡•á ‡§ï‡§ø ‡§π‡§Æ‡§æ‡§∞‡§æ ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§®‡§ú‡§∞‡§ø‡§Ø‡§æ ‡§®‡•Ä‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§Ö...  
3      Bhilli  ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§Ø‡•Ä ‡§ï‡•á‡§¶‡•Ç ‡§Ø‡•ã ‡§Ö‡§ü‡§≤ ‡§ú‡•Ä ‡§π‡•Ä ‡§π‡§§‡§æ ‡§ú‡§ø‡§®‡•ç ‡§π‡•Å‡§Ø‡•á ‡§¶...  
4      Bhilli  ‡§â‡§§‡•ç‡§∏‡§µ ‡§Æ‡§®‡§µ‡§æ ‡§®‡§æ ‡§â‡§™‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Æ‡§æ ‡§∏‡•Å‡§∞‡§æ‡§™‡§æ‡§® ‡§ï‡§∞‡§µ‡§æ ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ...  

üöÄ B·∫Øt ƒë·∫ßu post-processing to√†n b·ªô submission...
```

100%

‚Äá15999/15999‚Äá[00:03<00:00,‚Äá6937.61it/s]

```
‚úÖ DONE. Wrote post-processed submission to: /kaggle/working/submission_postprocessed.csv
```

|   | Row ID | Source Lang | Source Sentence                                                                           | Target Lang | Target Sentence                                                                           |
| - | ------ | ----------- | ----------------------------------------------------------------------------------------- | ----------- | ----------------------------------------------------------------------------------------- |
| 0 | 54334  | Hindi       | ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø 2014 ‡§ï‡•á ‡§¨‡§æ‡§¶, ‡§á‡§∏ ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•ã ‡§™‡•ç...         | Bhilli      | ‡§§‡§ø‡§®‡§æ‡§Ø‡•á ‡§ï‡•á‡§¶‡•Å ‡§ï‡•Ä 2014 ‡§®‡•á ‡§¨‡§æ‡§¶ ‡§Æ‡§æ ‡§á‡§®‡•ã ‡§ï‡§æ‡§Æ ‡§®‡•á ‡§™‡•ç‡§∞‡§ß‡§æ...         |
| 1 | 87641  | Hindi       | ‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø ‡§ï‡§†‡§ø‡§®‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§π‡§≤ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§ï‡•á...   | Bhilli      | ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§ï‡§†‡§ø‡§®‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§π‡§≤ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§ï‡•á ...    |
| 2 | 32543  | Hindi       | ‡§Æ‡•á‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§π‡•à ‡§ï‡§ø ‡§π‡§Æ‡§æ‡§∞‡•á ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§ï‡•ã‡§£, ‡§®‡•Ä‡§§‡§ø‡§Ø...   | Bhilli      | ‡§Æ‡§æ‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§∏‡•á ‡§ï‡§ø ‡§π‡§Æ‡§æ‡§∞‡§æ ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§®‡§ú‡§∞‡§ø‡§Ø‡§æ ‡§®‡•Ä‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§Ö...   |
| 3 | 26313  | Hindi       | ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡•á ‡§ï‡§π‡§æ ‡§Ø‡§π ‡§Ö‡§ü‡§≤ ‡§ú‡•Ä ‡§π‡•Ä ‡§•‡•á ‡§ú‡§ø‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¶‡•á‡§∂...     | Bhilli      | ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§Ø‡•Ä ‡§ï‡•á‡§¶‡•Ç ‡§Ø‡•ã ‡§Ö‡§ü‡§≤ ‡§ú‡•Ä ‡§π‡•Ä ‡§π‡§§‡§æ ‡§ú‡§ø‡§®‡•ç ‡§π‡•Å‡§Ø‡•á ‡§¶...      |
| 4 | 83303  | Hindi       | ‡§â‡§§‡•ç‡§∏‡§µ‡§æ‡§¶‡§ø ‡§Æ‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§â‡§™‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§∞‡§æ‡§™‡§æ‡§® ‡§ï‡§∞‡§®‡§æ ‡§∏‡§æ...  | Bhilli      | ‡§â‡§§‡•ç‡§∏‡§µ ‡§Æ‡§®‡§µ‡§æ ‡§®‡§æ ‡§â‡§™‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Æ‡§æ ‡§∏‡•Å‡§∞‡§æ‡§™‡§æ‡§® ‡§ï‡§∞‡§µ‡§æ ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ...   |
| 5 | 131411 | Hindi       | ‡§§‡•Å‡§Æ‡•ç‚Äç‡§π‡§æ‡§∞‡•á ‡§∏‡§æ‡§• ‡§ï‡§≠‡•Ä ‡§ê‡§∏‡§æ ‡§π‡•Å‡§Ü‡•§                                          | Bhilli      | ‡§§‡§Æ‡•Å ‡§®‡•Ä ‡§π‡§æ‡§§‡•á ‡§ï‡§¶‡•Ä ‡§ê‡§µ‡•Ä ‡§•‡§æ‡§à ‡•§                                              |
| 6 | 101809 | Hindi       | ‡§Ø‡§π ‡§∏‡§§‡•ç‡§∞ ‡§ó‡•ç‡§≤‡§æ‡§∏‡§ó‡•ã, ‡§Ø‡•Ç‡§®‡§æ‡§á‡§ü‡•á‡§° ‡§ï‡§ø‡§Ç‡§ó‡§°‡§Æ ‡§Æ‡•á‡§Ç ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§π‡•Å‡§Ü‡•§  | Bhilli      | ‡§Ø‡•ã ‡§∏‡§§‡•ç‡§∞ ‡§ó‡•ç‡§≤‡§æ‡§∏‡§ó‡•ã, ‡§Ø‡•Ç‡§®‡§æ‡§á‡§ü‡•á‡§° ‡§ï‡§ø‡§Ç‡§ó‡§°‡§Æ ‡§Æ‡§æ ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§•‡§æ‡§Ø‡•ã‡•§  |
| 7 | 59328  | Hindi       | ‡§Ø‡§π 9 ‡§Æ‡§æ‡§∞‡•ç‡§ö 2012 ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§π‡•Å‡§à ‡§•‡•Ä, ‡§ú‡§ø‡§∏‡•á ‡§Ü‡§Æ ‡§§‡•å‡§∞ ...            | Bhilli      | ‡§Ø‡•ã ‡•Ø ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•® ‡§Æ‡§æ ‡§∞‡§ø‡§≤‡§ø‡§ú ‡§•‡§æ‡§à ‡§•‡•Ä, ‡§ú‡§ø‡§®‡•á ‡§Ü‡§Æ ‡§§‡•å‡§∞ ‡§™...       |
| 8 | 57205  | Hindi       | ‡§®‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡§ø ‡§Æ‡§æ‡§Æ‡§≤‡•ã‡§Ç ‡§ï‡•Ä ‡§Æ‡§Ç‡§§‡•ç‡§∞‡§ø‡§Æ‡§Ç‡§°‡§≤‡•Ä‡§Ø ‡§∏‡§Æ‡§ø‡§§‡§ø ‡§®‡•á ‡§≠‡§æ‡§∞‡§§‡•Ä... | Bhilli      | ‡§®‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡§ø ‡§Æ‡§æ‡§Æ‡§≤‡•ã‡§Ç ‡§®‡•Ä ‡§Æ‡§Ç‡§§‡•ç‡§∞‡§ø‡§Æ‡§Ç‡§°‡§≤‡•Ä‡§Ø ‡§∏‡§Æ‡§ø‡§§‡§ø ‡§®‡•á ‡§≠‡§æ‡§∞‡§§‡•Ä... |
| 9 | 103641 | Hindi       | ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•ã ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§æ‡§´-‡§∏‡•Å‡§•‡§∞‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§î‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ ‡§∏‡•ç‡§µ‡§ö‡•ç...    | Bhilli      | ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•ã ‡§ì‡§°‡§º‡•ã‡§É ‡§á‡§∏‡•Å ‡§™‡•Å‡§∞‡§Ö‡§É ‡§∏‡§æ‡§´-‡§∏‡•Å‡§•‡§∞‡§æ ‡§¨‡§á‡§ì‡§É ‡§ì‡§°‡§º‡§ì ‡§á‡§∏...    |
